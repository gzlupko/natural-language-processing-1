---
title: "NLP"
author: "Gian Zlupko"
---

## Libraries
```{r}
#Make sure you install and load the following libraries

library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(tidyverse) #You will need the full tidyverse package not tidyr and dyplr separately
library(topicmodels)

#IF USING A MAC PLEASE RUN THIS CODE
Sys.setlocale("LC_ALL", "C")
```

## In the class-notes folder you will find real csv files exported from real student's note taking in this class. Import all document files and the list of weeks file
```{r}
library(tidyverse)

#Create a list of all the files, then loop over file list importing them and binding them together

getwd()

D1 <- list.files(path = "/Users/gianzlupko/Desktop/HUDK 4051 Learning Analytics/Assn2 - NLP/natural-language-processing/class-notes",
               pattern = "*.csv", 
               full.names = T) %>% 
    map_df(~read_csv(., col_types = cols(.default = "c"))) 

  
```

## Step 1 - Clean
```{r}
#Separate out the variables of interest
D1 <- select(D1, Title, Notes)

#Remove the htlm tags from your text
D1$Notes <- gsub("<.*?>", "", D1$Notes)
D1$Notes <- gsub("nbsp", "" , D1$Notes)
D1$Notes <- gsub("nbspnbspnbsp", "" , D1$Notes)
D1$Notes <- gsub("<U+00A0><U+00A0><U+00A0>", "" , D1$Notes)

#Merge the weeks data with your notes data so that each line has a week attributed to it 

week_list <- read_csv("week-list.csv") 
D1 <- merge(D1, week_list, by.x = "Title", by.y = "Title", all.x = TRUE)

#Also remove readings not belonging to the class (IE - that are NA for week)
D1 <- D1 %>%
  drop_na(week) 

```

## Step 2 - Process text using the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- VCorpus(VectorSource(D1$Notes))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)

#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus <- TermDocumentMatrix(corpus)

#Note: we won't remove plural words here, plural words in English tend to be highly irregular and difficult to extract reliably
```

What processing steps have you conducted here? Why is this important? Are there any other steps you should take to process your text before analyzing?

Functions from the tm library were called to remove text data that is not needed for topic modeling. Specifically, stop words, punctuation, and numbers were removed. In addition, words were convereted to their stems to ensure that future queries of word stems are more inclusive than if the words were left un-stemmed. 


## Step 3 - Find common words
```{r}
#The tm package can do some simple analysis, like find the most common words
findFreqTerms(tdm.corpus, lowfreq=500, highfreq=Inf)
#We can also create a vector of the word frequencies that can be useful to see common and uncommon words
word.count <- sort(rowSums(as.matrix(tdm.corpus)), decreasing=TRUE)
word.count <- data.frame(word.count)
#Look at the word.count dataframe
View(word.count)
```

## Generate a Word Cloud

### ColorBrewer
ColorBrewer is a useful tool to help you choose colors for visualizations that was originally built for cartographers. On the ColorBrewer website (http://colorbrewer2.org/#) you can test different color schemes or see what their preset color schemes look like. This is very useful, especially if you are making images for colorblind individuals. 
```{r}
#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud, make sure your window is large enough to see it
wordcloud(corpus, min.freq=500, scale=c(5,2),rot.per = 0.25,
          random.color=T, max.word=45, random.order=F,colors=col)
```

# Sentiment Analysis

### Match words in corpus to lexicons of positive & negative words
```{r}
#Upload positive and negative word lexicons
positive <- readLines("positive-words.txt")
negative <- readLines("negative-words.txt")

#Search for matches between each word and the two lexicons
D1$positive <- tm_term_score(tdm.corpus, positive)
D1$negative <- tm_term_score(tdm.corpus, negative)

#Generate an overall pos-neg score for each line
D1$score <- D1$positive - D1$negative

```

## Using ggplot Generate a visualization of the mean sentiment score over weeks, remove rows that have readings from other classes (NA for weeks). You will need to summarize your data to achieve this.
```{r}

tidy_D1 <- D1 %>%
  drop_na(Notes) 

D1 <- D1 %>%
  drop_na(week) 

tidy_D1 %>%
  count(week) 

tidy_D1 %>%
  group_by(week) %>%
  summarize(avg_score = mean(score)) %>%
  ggplot(aes(x = week, y = avg_score)) + geom_bar(stat = "identity") +
  ylab("Mean Sentiment Score") + xlab("Week") + ggtitle("Mean Sentiment Score Per Week") + scale_fill_brewer(palette = "Dark2") 



# Regression analysis
sentiment_mod <- lm(score ~ week, data = tidy_D1) 
summary(sentiment_mod) 


```

# LDA Topic Modelling

Using the same csv file you have generated the LDA analysis will treat each row of the data frame as a document. Does this make sense for generating topics?

Yes because each row was generated from a previously separate document containing students' notes. 


```{r}
#Term Frequency Inverse Document Frequency
dtm.tfi <- DocumentTermMatrix(corpus, control = list(weighting = weightTf))

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1]

#Remove non-zero entries
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
dtm.tfi2   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows


#Identify rows with zero entries
which(rowTotals %in% c(0))
no_notes <- which(rowTotals %in% c(0))

#Remove these rows from original dataset
#D1 <- D1[-c(?,?),]
D1_filtered <- D1[-no_notes, ]



#Generate LDA model, k is the number of topics and the seed is a random number to start the process
lda.model = LDA(dtm.tfi2, k = 5, seed = 150)

#Which terms are most common in each topic
terms(lda.model, k = 10) 

#Identify which documents belong to which topics based on the notes taken by the student
D1_filtered$topic<- topics(lda.model)

```

What does an LDA topic represent? 

An LDA topic represents a probability disribution of words found within a given corpus. Each LDA topic is calculated from the combination of multiple probability distributions including the distribution of a word in a document, the distribution of a topic within a document, and the distribution of topics over similar documents. Together, the LDA topic therefore represents a probability distribution of probability distributions. 


# Final Task 

Find a set of documents, perhaps essays you have written or articles you have available and complete an LDA analysis of those documents. Does the method group documents as you would expect?

Somewhat - however a quick glance at the topics is hard to discern the relationship between the words and the topics. This corpus consisted of only four research articles (~100 pages). It would be interesting to run LDA on a similar grouping of articles from within the same area of research but on a larger corpus. Does interpretability of terms and their corresponding topics generally increase when more is fed into the LDA? If so, similar to what personality researchers do for survey item creation, you could have a team of researchers rate the words as similar to the latent variable in which they correspond or not and, from those ratings, caculate an inter-rater reliability. Ostensibly, the greater the inter-rater reliability, the more agreement there would be that the topic corresponds with a shared cognitive or social construct.



```{r}

library(pdftools)

research_articles <- list.files(pattern = "pdf$") 
class(research_articles)

# create list object with text data from PDF files 
articles <-lapply(research_articles, pdf_text)

# remove lecture slides, retain only 4 research articles for analysis 
articles <- articles[-5]
length(articles)


# show number of pages for each PDF file using length
lapply(articles, length) 

# begin text cleaning 
articles <- gsub("<.*?>", "", articles)
articles <- gsub("nbsp", "" , articles)
articles <- gsub("nbspnbspnbsp", "" , articles)
articles <- gsub("<U+00A0><U+00A0><U+00A0>", "" , articles)


# create corpus for articles 
articles_corpus <- VCorpus(VectorSource(articles))


#Remove spaces
articles_corpus <- tm_map(articles_corpus, stripWhitespace)
#Convert to lower case
articles_corpus <- tm_map(articles_corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
articles_corpus <- tm_map(articles_corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info 
articles_corpus <- tm_map(articles_corpus, stemDocument)
#Remove numbers
articles_corpus <- tm_map(articles_corpus, removeNumbers)
#remove punctuation
articles_corpus <- tm_map(articles_corpus, removePunctuation)
#Convert to plain text for mapping by wordcloud package
articles_corpus <- tm_map(articles_corpus, PlainTextDocument, lazy = TRUE)

# create term frequency matrix for research articles 
dtm.tfi_articles <- DocumentTermMatrix(articles_corpus,control = list(weighting = weightTf)) 


```

LDA topic modeling for research articles 

```{r}

# generate LDA with 5 topics 
lda_model_articles <- LDA(dtm.tfi_articles, k = 5, seed = 81) 

# view most common terms in each topic 
terms(lda_model_articles, k = 10) 

articles_corpus$topics <- topics(lda_model_articles) 

# Run subsequent LDA again with steps to remove non-zero entries
#Remove very uncommon terms (term freq inverse document freq < 0.1)
articles_1 <- dtm.tfi_articles[, dtm.tfi_articles$v >= 0.1]

#Remove non-zero entries
article_row_sums <- apply(articles_1 , 1, sum) 
articles_2   <- articles_1[article_row_sums> 0, ] 


# Highlight rows with zero entries
articles_zero_rows<-which(articles_row_sums %in% c(0))
articles_zero_rows

#Generate LDA model, k is the number of topics and the seed is a random number to start the process
articles_2.model = LDA(articles_2, k = 5, seed = 751)
terms(articles_2.model)
articles$topics <- topics(articles_2.model)

# view topics 
View(articles$topics)


```


Research articles corpus EDA 

```{r}


# create word cloud for research articles 

#Define the colors the cloud will use
col=brewer.pal(6,"Dark2")
#Generate cloud, make sure your window is large enough to see it
wordcloud(articles_corpus, min.freq=500, scale=c(3,1),rot.per = 0.25,
          random.color=T, max.word=15, random.order=F,colors=col)


# descriptive statistics for articles corpus 

word.count.articles <- sort(rowSums(as.matrix(tdm.corpus.articles)), decreasing=TRUE)

word.count.articles <- data.frame(word.count.articles)
head(word.count.articles)

```






```

